# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sm3FaFFyYXZ9R-i2w5S4oRRnJ8LlCHW-

# ***1. 데이터 불러오기***
"""

# root-path 지정
from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

# import data
phi = pd.read_csv('/content/gdrive/MyDrive/crimefin/total.csv', sep=',', encoding='utf-8', header=None)  #피싱 데이터
non_phi = pd.read_csv('/content/gdrive/MyDrive/crimefin/non-total.csv', sep=',', encoding='utf-8', header=None)  #피싱 아닌 데이터

# split data
x_phi = phi[4]
y_phi = phi[0]
x_non_phi = non_phi[4]
y_non_phi = non_phi[0]

x_phi.head()

# 데이터 확인
y_non_phi.head()

"""# **2. train, test로 데이터 분리**"""

from sklearn.model_selection import train_test_split

# 데이터 분리
x_phi_train, x_phi_test, y_phi_train, y_phi_test = train_test_split(x_phi, y_phi, test_size=0.1, shuffle=True, random_state=1)
x_non_phi_train, x_non_phi_test, y_non_phi_train, y_non_phi_test = train_test_split(x_non_phi, y_non_phi, test_size=0.1, shuffle=True, random_state=1)

x_train = pd.concat([x_phi_train, x_non_phi_train])
x_test = pd.concat([x_phi_test, x_non_phi_test])
y_train = pd.concat([y_phi_train, y_non_phi_train])
y_test = pd.concat([y_phi_test, y_non_phi_test])
print(x_train.head())
print(x_train.shape)

# 분리된 데이터 확인
train_data = pd.concat([y_train, x_train], axis=1)
test_data = pd.concat([y_test, x_test], axis=1)

print(train_data)
#print(test_data.head())
print(train_data.shape)
print(test_data.shape)

"""# **3. 토큰화 및 특수 기호 제거**"""

!pip install unidic-lite  # 필요한 경우 unidic-lite를 설치합니다.
!pip install mecab-python3

!apt-get update
!apt-get install g++ openjdk-8-jdk
!pip3 install JPype1-py3

# Install Mecab
!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

!pip install konlpy

from konlpy.tag import Mecab
import re

mecab = Mecab()

def mecab_tokenize(text):
    """
        Mecab을 사용하여 tokenized text 반환
    """
    return " ".join(mecab.morphs(text))

def clean_etc_reg_ex(title):
    """
        정규식을 통해 기타 공백과 기호, 숫자등을 제거
    """
    title = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"]', '', title) #remove punctuation
    title = re.sub(r'[∼%①②⑤⑪…→·]', '', title)
    title = re.sub(r'\d+', '', title) #remove number
    title = re.sub(r'\s+', ' ', title) #remove extra space
    title = re.sub(r'<[^>]+>','',title) #remove Html tags
    title = re.sub(r'\s+', ' ', title) #remove spaces
    title = re.sub(r"^\s+", '', title) #remove space from start
    title = re.sub(r'\s+$', '', title) #remove space from the end
    title = re.sub("[一-龥]",'', title)
    return title

def label_to_idx(label, dic):
    return dic[label]

def slice_from_behind(text, num_of_chars):
    return text[-num_of_chars:]

# 특수기호 제거
train_data[4] = train_data[4].apply(str).apply(clean_etc_reg_ex)
test_data[4] = test_data[4].apply(str).apply(clean_etc_reg_ex)

# label to idx
#train_data[0] = train_data[0].apply(label_to_idx, dic={"1":"1", "0":"0"})
#test_data[0] = test_data[0].apply(label_to_idx, dic={"1":"1", "0":"0"})

# mecab tokenize
train_data[4] = train_data[4].apply(str).apply(mecab_tokenize)
test_data[4] = test_data[4].apply(str).apply(mecab_tokenize)

# SLICE
train_data[4] = train_data[4].apply(str).apply(slice_from_behind, num_of_chars=500)
test_data[4] = test_data[4].apply(str).apply(slice_from_behind, num_of_chars=500)

test_data.head()

"""# **4. tsv 파일로 저장**"""

# crimefin 폴더에 tsv 파일 영구 저장 (구드 마운트 후 실행해야함)
train_data.to_csv('/content/gdrive/MyDrive/crimefin/train.tsv', sep='\t', encoding='utf-8', index=False, header=None)
test_data.to_csv('/content/gdrive/MyDrive/crimefin/test.tsv', sep='\t', encoding='utf-8', index=False, header=None)

# 현재 위치 확인
import os
print(os.getcwd())